--- /usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py
+++ /usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py
@@ -1,4 +1,4 @@
-class DataParallel(Module, Generic[T]):
+class DataParallel(Module):
     r"""Implements data parallelism at the module level.
 
     This container parallelizes the application of the given :attr:`module` by
@@ -10,10 +10,7 @@
 
     The batch size should be larger than the number of GPUs used.
 
-    .. warning::
-        It is recommended to use :class:`~torch.nn.parallel.DistributedDataParallel`,
-        instead of this class, to do multi-GPU training, even if there is only a single
-        node. See: :ref:`cuda-nn-ddp-instead` and :ref:`ddp`.
+    See also: :ref:`cuda-nn-dataparallel-instead`
 
     Arbitrary positional and keyword inputs are allowed to be passed into
     DataParallel but some types are specially handled. tensors will be
@@ -73,95 +70,62 @@
 
     Example::
 
-        >>> # xdoctest: +SKIP
         >>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])
         >>> output = net(input_var)  # input_var can be on any device, including CPU
     """
 
     # TODO: update notes/cuda.rst when this class handles 8+ GPUs well
 
-    def __init__(
-        self,
-        module: T,
-        device_ids: Optional[Sequence[Union[int, torch.device]]] = None,
-        output_device: Optional[Union[int, torch.device]] = None,
-        dim: int = 0,
-    ) -> None:
-        super().__init__()
-        torch._C._log_api_usage_once("torch.nn.parallel.DataParallel")
-        device_type = _get_available_device_type()
-        if device_type is None or device_type == "mps":
+    def __init__(self, module, device_ids=None, output_device=None, dim=0):
+        super(DataParallel, self).__init__()
+
+        if not torch.cuda.is_available():
             self.module = module
             self.device_ids = []
             return
 
         if device_ids is None:
-            device_ids = _get_all_device_indices()
-
-        if device_ids is None:
-            raise RuntimeError("no available devices were found")
-
+            device_ids = list(range(torch.cuda.device_count()))
         if output_device is None:
             output_device = device_ids[0]
 
         self.dim = dim
         self.module = module
-        self.device_ids = [_get_device_index(x, True) for x in device_ids]
+        self.device_ids = list(map(lambda x: _get_device_index(x, True), device_ids))
         self.output_device = _get_device_index(output_device, True)
-        self.src_device_obj = torch.device(device_type, self.device_ids[0])
+        self.src_device_obj = torch.device("cuda:{}".format(self.device_ids[0]))
 
-        if device_type == "cuda":
-            _check_balance(self.device_ids)
+        _check_balance(self.device_ids)
 
         if len(self.device_ids) == 1:
-            self.module.to(self.src_device_obj)
+            self.module.cuda(device_ids[0])
 
-    def forward(self, *inputs: Any, **kwargs: Any) -> Any:
-        with torch.autograd.profiler.record_function("DataParallel.forward"):
-            if not self.device_ids:
-                return self.module(*inputs, **kwargs)
+    def forward(self, *inputs, **kwargs):
+        if not self.device_ids:
+            return self.module(*inputs, **kwargs)
 
-            for t in chain(self.module.parameters(), self.module.buffers()):
-                if t.device != self.src_device_obj:
-                    raise RuntimeError(
-                        "module must have its parameters and buffers "
-                        f"on device {self.src_device_obj} (device_ids[0]) but found one of "
-                        f"them on device: {t.device}"
-                    )
+        for t in chain(self.module.parameters(), self.module.buffers()):
+            if t.device != self.src_device_obj:
+                raise RuntimeError("module must have its parameters and buffers "
+                                   "on device {} (device_ids[0]) but found one of "
+                                   "them on device: {}".format(self.src_device_obj, t.device))
 
-            inputs, module_kwargs = self.scatter(inputs, kwargs, self.device_ids)
-            # for forward function without any inputs, empty list and dict will be created
-            # so the module can be executed on one device which is the first one in device_ids
-            if not inputs and not module_kwargs:
-                inputs = ((),)
-                module_kwargs = ({},)
+        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)
+        if len(self.device_ids) == 1:
+            return self.module(*inputs[0], **kwargs[0])
+        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
+        outputs = self.parallel_apply(replicas, inputs, kwargs)
+        return self.gather(outputs, self.output_device)
 
-            if len(self.device_ids) == 1:
-                return self.module(*inputs[0], **module_kwargs[0])
-            replicas = self.replicate(self.module, self.device_ids[: len(inputs)])
-            outputs = self.parallel_apply(replicas, inputs, module_kwargs)
-            return self.gather(outputs, self.output_device)
+    def replicate(self, module, device_ids):
+        return replicate(module, device_ids)
 
-    def replicate(
-        self, module: T, device_ids: Sequence[Union[int, torch.device]]
-    ) -> List[T]:
-        return replicate(module, device_ids, not torch.is_grad_enabled())
-
-    def scatter(
-        self,
-        inputs: Tuple[Any, ...],
-        kwargs: Optional[Dict[str, Any]],
-        device_ids: Sequence[Union[int, torch.device]],
-    ) -> Any:
+    def scatter(self, inputs, kwargs, device_ids):
         return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)
 
-    def parallel_apply(
-        self, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any
-    ) -> List[Any]:
-        return parallel_apply(
-            replicas, inputs, kwargs, self.device_ids[: len(replicas)]
-        )
+    def parallel_apply(self, replicas, inputs, kwargs):
+        return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
 
-    def gather(self, outputs: Any, output_device: Union[int, torch.device]) -> Any:
+    def gather(self, outputs, output_device):
         return gather(outputs, output_device, dim=self.dim)
 